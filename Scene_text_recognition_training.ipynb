{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scene text recognition training",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Q9IUObdgAF",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains training code for recognising text from images.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBKTzi3rbQHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import all libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pathlib import Path\n",
        "from skimage import io, transform\n",
        "from PIL import Image\n",
        "import copy,os,sys\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "import string\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IGVh1OHjk05",
        "colab_type": "code",
        "outputId": "ffa07261-428e-497f-a26d-f249a064eea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU0hREy6eYBs",
        "colab_type": "text"
      },
      "source": [
        "Download the dataset and extract into Google drive to access with colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMESbipbcR-0",
        "colab_type": "code",
        "outputId": "bddf67d3-d5d8-4ac3-9878-ed50f73155f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!wget https://www.robots.ox.ac.uk/~vgg/data/text/mjsynth.tar.gz\n",
        "!tar -zxf '/content/mjsynth.tar.gz'\n",
        "!mkdir '/content/drive/My Drive/Scene text recognition/data'\n",
        "!cp -r /content/mnt/ramdisk/max/90kDICT32px/[1]* '/content/drive/My Drive/Scene text recognition/data'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-01 08:46:03--  https://www.robots.ox.ac.uk/~vgg/data/text/mjsynth.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10678411583 (9.9G) [application/x-gzip]\n",
            "Saving to: ‘mjsynth.tar.gz’\n",
            "\n",
            "mjsynth.tar.gz      100%[===================>]   9.94G  11.5MB/s    in 14m 55s \n",
            "\n",
            "2019-07-01 09:01:00 (11.4 MB/s) - ‘mjsynth.tar.gz’ saved [10678411583/10678411583]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07Zhep8l3dZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move the annotation file to Google drive\n",
        "!cp '/content/mnt/ramdisk/max/90kDICT32px/annotation.txt' '/content/drive/My Drive/Scene text recognition/annotation.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nTCCrq0iSNJ",
        "colab_type": "code",
        "outputId": "af9bf96f-1f8d-4b07-a6ab-62becf2abf9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "!tail '/content/mnt/ramdisk/max/90kDICT32px/annotation.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./1/1/10_besots_7149.jpg 7149\n",
            "./1/1/9_Sensibly_69429.jpg 69429\n",
            "./1/1/8_Sandra_67698.jpg 67698\n",
            "./1/1/7_Bombastic_8610.jpg 8610\n",
            "./1/1/6_embracing_25311.jpg 25311\n",
            "./1/1/5_minion_48634.jpg 48634\n",
            "./1/1/4_underbellies_82078.jpg 82078\n",
            "./1/1/3_JERKIER_41423.jpg 41423\n",
            "./1/1/2_Senoritas_69404.jpg 69404\n",
            "./1/1/1_pontifically_58805.jpg 58805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMLNW552GcwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all letters, digits and punctutation from the string class\n",
        "labels=string.punctuation+string.ascii_letters+string.digits\n",
        "labels=list(labels)+['<sos>','<eos>']\n",
        "\n",
        "label2idx={val:i for i,val in enumerate(labels)}\n",
        "idx2label={i:val for i,val in enumerate(labels)}\n",
        "vocab_size=len(label2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Ile_v7HFSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grads = {}\n",
        "def save_grad(name):\n",
        "    def hook(grad):\n",
        "        grads[name] = grad\n",
        "    return hook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh2v9CSWwuiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(x,mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]):\n",
        "    mean=torch.tensor(mean)\n",
        "    std=torch.tensor(std)\n",
        "    return (x-mean)/std\n",
        "    \n",
        "def denormalize(x,mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5]):\n",
        "    mean=torch.tensor(mean)\n",
        "    std=torch.tensor(std)\n",
        "    return x*std+mean\n",
        "    \n",
        "# changes dim of x from (w,h,3)->(1,3,w,h) to easily pass through model\n",
        "def img2passable(x):\n",
        "    x=x.permute((2,0,1))\n",
        "    x=x.unsqueeze(0)\n",
        "    return x\n",
        "\n",
        "# inverse of img_to_passable\n",
        "# converts (1,3,w,h) to (w,h,3)\n",
        "def passable2img(x):\n",
        "    x=x.squeeze(0)\n",
        "    x=x.permute((1,2,0))\n",
        "    return x\n",
        "\n",
        "# x is list of labels and output is list of indices\n",
        "def labels2idx(x):\n",
        "    return [label2idx[i.item()] for i in x]\n",
        "\n",
        "def idx2labels(x):\n",
        "    return [idx2label[i.item()] for i in x]\n",
        "\n",
        "# x is list of indices\n",
        "# returns string of text\n",
        "def get_label(x):\n",
        "    return ''.join(idx2labels(x)[:-1])  # Removes <eos>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdnMmajFd3u0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make all models\n",
        "\n",
        "# Create residual block\n",
        "class unit(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(unit,self).__init__()\n",
        "        \n",
        "        self.BN1=nn.BatchNorm2d(in_channels)\n",
        "        self.BN2=nn.BatchNorm2d(out_channels)\n",
        "        self.conv1x1=nn.Conv2d(in_channels, out_channels, 1)\n",
        "        self.conv3x3=nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        y=self.conv1x1(F.relu(self.BN1(x)))\n",
        "        y=self.conv3x3(F.relu(self.BN2(y)))\n",
        "        # y=self.conv1x1(F.relu(x))\n",
        "        # y=self.conv3x3(F.relu(y))\n",
        "        # x=torch.cat((x,y),dim=1)\n",
        "        x=x+y\n",
        "        return x\n",
        "    \n",
        "# Create encoder CNN\n",
        "class Encoder_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder_CNN,self).__init__()\n",
        "        self.b0=nn.Conv2d(3,32,3,padding=1)\n",
        "\n",
        "        self.conv01=nn.Conv2d(32,32,3,stride=2,padding=1)\n",
        "        self.b1=nn.Sequential(unit(32,32),unit(32,32),unit(32,32))\n",
        "        \n",
        "        self.conv12=nn.Conv2d(32,64,3,stride=2,padding=1)\n",
        "        self.b2=nn.Sequential(unit(64,64),unit(64,64),unit(64,64),unit(64,64))\n",
        "                \n",
        "        self.conv23=nn.Conv2d(64,128,3,stride=(2,1),padding=1)\n",
        "        self.b3=nn.Sequential(unit(128,128),unit(128,128),unit(128,128),unit(128,128),unit(128,128),unit(128,128))\n",
        "        \n",
        "        self.conv34=nn.Conv2d(128,256,3,stride=(2,1),padding=1)\n",
        "        self.b4=nn.Sequential(unit(256,256),unit(256,256),unit(256,256),unit(256,256),unit(256,256),unit(256,256))\n",
        "        \n",
        "        self.conv45=nn.Conv2d(256,512,3,stride=(2,1),padding=1)\n",
        "        self.b5=nn.Sequential(unit(512,512),unit(512,512),unit(512,512))\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x=self.b0(x)\n",
        "        x=self.b1(self.conv01(F.relu(x)))\n",
        "        x=self.b2(self.conv12(F.relu(x)))\n",
        "        x=self.b3(self.conv23(F.relu(x)))\n",
        "        x=self.b4(self.conv34(F.relu(x)))\n",
        "        x=self.b5(self.conv45(F.relu(x)))\n",
        "        return x\n",
        "        \n",
        "# Create decoder LSTM\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder,self).__init__()\n",
        "        \n",
        "        embed_size=round(vocab_size**0.25)\n",
        "\n",
        "        self.lstm_cell=nn.LSTMCell(embed_size+512,512) \n",
        "        self.embeddings=nn.Embedding(vocab_size,embed_size)\n",
        "        self.ench=nn.Linear(512,256)   # V in paper\n",
        "        self.dech=nn.Linear(512,256)   # W in paper\n",
        "        self.w=nn.Linear(256,1,bias=False)\n",
        "        self.lin1=nn.Linear(512,256)\n",
        "        self.linout=nn.Linear(256,vocab_size)\n",
        "            \n",
        "            \n",
        "    # enc_h is all encoder hidden states (N,seq_len,512)\n",
        "    # h is last hidden state of encoder for all layers (num_layers*directions,N,256)\n",
        "    # c is last cell state of encoder for all layers (num_layers*directions,N,256)\n",
        "    def forward(self,enc_h,h,c,labels,lengths):\n",
        "        h=h[2:].transpose(0,1).contiguous()   # take last layer only\n",
        "        h=h.view(h.size(0),-1)    # (N,512)\n",
        "        c=c[2:].transpose(0,1).contiguous()\n",
        "        c=c.view(c.size(0),-1)    # (N,512)\n",
        "        \n",
        "        max_length=max(lengths).item()\n",
        "        batch_size=len(labels)\n",
        "        \n",
        "        inp=torch.ones((batch_size),dtype=torch.long,device=device)*label2idx['<sos>']  # input at t=1 is <sos> \n",
        "        out_list,alpha_list=[],[]\n",
        "        for t in range(max_length):\n",
        "            inp=self.embeddings(inp)\n",
        "            g,alpha=self.step(enc_h,h)\n",
        "            h,c=self.lstm_cell(torch.cat((inp,g),dim=1),(h,c))\n",
        "            out=self.linout(F.relu(self.lin1(h)))\n",
        "            \n",
        "            inp=labels[:,t]                       # previous output is current input\n",
        "            out_list.append(out.unsqueeze(1))     # adding time dimension\n",
        "            alpha_list.append(alpha.unsqueeze(1)) # adding time dimension\n",
        "            \n",
        "        out_list=torch.cat(out_list,dim=1)   \n",
        "        alpha_list=torch.cat(alpha_list,dim=1)   \n",
        "        # out_list=F.log_softmax(out_list,dim=2)\n",
        "        return out_list,alpha_list\n",
        "    \n",
        "    # generates glimpse vector between decoder's prev hidden state and all encoder hidden states for a single t\n",
        "    def step(self,enc_h,h):\n",
        "        h=h.unsqueeze(1)\n",
        "        e=self.w(torch.tanh(self.dech(h)+self.ench(enc_h)))\n",
        "        e=e.squeeze(-1)\n",
        "        alpha=torch.softmax(e,dim=1)\n",
        "        g=torch.einsum('bi,bij->bj',alpha,enc_h)\n",
        "        return g, alpha\n",
        "        \n",
        "# Combines both the CNN encoder and LSTM decoder to produce final output \n",
        "class Recognition(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Recognition,self).__init__()\n",
        "        \n",
        "        self.enc_cnn=Encoder_CNN()\n",
        "        self.enc_rnn=nn.LSTM(512,256,num_layers=2,batch_first=True,bidirectional=True)\n",
        "#         self.lin1=nn.Linear(512,256)\n",
        "        self.decoder1=Decoder()\n",
        "        self.decoder2=Decoder()    \n",
        "        \n",
        "    def forward(self,x,flabels,rlabels,lengths):\n",
        "        x=self.enc_cnn(x)\n",
        "        x=x.permute((0,3,1,2)).squeeze(-1)   # reshapes to (batch_size,seq_length,feature_length)\n",
        "        out,(h,c) =self.enc_rnn(x)\n",
        "        fout_list, falpha_list=self.decoder1(out,h,c,flabels,lengths)      # forward decoder\n",
        "        rout_list, ralpha_list=self.decoder2(out,h,c,rlabels,lengths)      # backward decoder\n",
        "#         mask=self.create_mask(lengths)\n",
        "#         output_list[~mask]=0  #np.float('-inf')\n",
        "#         out=F.relu(self.lin1(out))     # try batch norm here \n",
        "        return fout_list, rout_list, falpha_list\n",
        "    \n",
        "    def create_mask(self,lengths):\n",
        "        mask=torch.arange(max(lengths)).expand((len(lengths),-1)).to(device)\n",
        "        mask=(mask<lengths.unsqueeze(-1))\n",
        "        return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz3uZySI-BFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(I,model):\n",
        "    # One time cost only \n",
        "    x=model.enc_cnn(img2passable(I))\n",
        "    x=x.permute((0,3,1,2)).squeeze(-1)    # reshapes to (batch_size,seq_length,feature_length)\n",
        "    enc_h,(h,c) = model.enc_rnn(x)\n",
        "    h=h[2:].transpose(0,1).contiguous()   # take last layer only\n",
        "    h=h.view(h.size(0),-1)    # (1,512)\n",
        "    c=c[2:].transpose(0,1).contiguous()\n",
        "    c=c.view(c.size(0),-1)    # (1,512)\n",
        "    inp=torch.ones((1),dtype=torch.long, device=device)*label2idx['<sos>']  # input at t=1 is <sos> \n",
        "    return inp,h,c\n",
        "\n",
        "\n",
        "def decode(model,inp,h,c):\n",
        "    inp=model.decoder1.embeddings(inp)\n",
        "    g,_=model.decoder1.step(enc_h,h)\n",
        "    h,c=model.decoder1.lstm_cell(torch.cat((inp,g),dim=1),(h,c))\n",
        "    out=model.decoder1.linout(F.relu(model.decoder1.lin1(h)))\n",
        "    return out, h, c\n",
        "\n",
        "# Decodes the given sequence logits\n",
        "# pred=(seq_len,vocab_size), k=beam width\n",
        "# if k=1 => greedy \n",
        "# if 1<k<t => beam search  \n",
        "# if k=t => brute-force \n",
        "def decoder(pred,k=1):    \n",
        "    if k==1:                   # Greedy Search   # TODO: make it for input as image only\n",
        "        for t in range(pred.size(0)):\n",
        "            i=torch.argmax(pred[t])\n",
        "            if idx2label[i.item()]!='<eos>':\n",
        "                print(idx2label[i.item()],end='')\n",
        "\n",
        "    elif k!=pred.size(0):      # Beam Search TODO\n",
        "        inp,h,c=preprocess(I,model)\n",
        "        out,h,c=decode(model,inp,h,c)\n",
        "        val,ind=torch.topk(out,k,largest=True)\n",
        "        hist=[]\n",
        "        labels=idx2labels(ind)\n",
        "        for i in range(len(labels)):\n",
        "            hist.append((labels[i],val[i],h,c))  # label,prob,h,c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sxIt8p6mh7y",
        "colab_type": "text"
      },
      "source": [
        "## Start TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQaM76eTEb9E",
        "colab_type": "text"
      },
      "source": [
        "Change the image and model paths below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ckVIWqkd3Zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transforms=transforms.Compose([\n",
        "        transforms.Resize((32,100)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])])\n",
        "\n",
        "valid_transforms=transforms.Compose([\n",
        "        transforms.Resize((32,100)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])])\n",
        "\n",
        "def my_collate(batch):\n",
        "    data=[i[0].unsqueeze(0) for i in batch]\n",
        "    flabels=[i[1] for i in batch]\n",
        "    rlabels=[i[2] for i in batch]\n",
        "    lengths=[i[3] for i in batch]\n",
        "    flabels=torch.LongTensor([i+[label2idx['<eos>']]*(max(lengths)-len(i)) for i in flabels])   # pad with <eos> tokens\n",
        "    rlabels=torch.LongTensor([i+[label2idx['<sos>']]*(max(lengths)-len(i)) for i in rlabels])   # pad with <sos> tokens\n",
        "    return torch.cat(data,dim=0), flabels, rlabels, torch.LongTensor(lengths)\n",
        "    \n",
        "class dataset(datasets.ImageFolder):\n",
        "    def __getitem__(self, i):\n",
        "        original_tuple = super(dataset, self).__getitem__(i)\n",
        "        path = self.imgs[i][0]\n",
        "        label=path.split('_')[1]\n",
        "        flabel=list(label)+['<eos>']          # forward output labels\n",
        "        rlabel=list(label)[::-1]+['<sos>']    # reverse output labels\n",
        "        flabel=[label2idx[i] for i in flabel]\n",
        "        rlabel=[label2idx[i] for i in rlabel]\n",
        "        return original_tuple[0],flabel,rlabel,len(flabel)\n",
        "\n",
        "\n",
        "IMG_PATH='/content/drive/My Drive/Scene text recognition/data/'\n",
        "train_set=dataset(IMG_PATH, train_transforms)\n",
        "valid_set=dataset(IMG_PATH, valid_transforms)\n",
        "# test_set=dataset('/content/images/',test_df,valid_transforms,istest=True)\n",
        "\n",
        "batch_size=64\n",
        "train_loader=DataLoader(train_set, batch_size=batch_size, shuffle=True,collate_fn=my_collate,pin_memory=True,num_workers=8)\n",
        "valid_loader=DataLoader(valid_set, batch_size=batch_size)\n",
        "# test_loader=DataLoader(test_set, batch_size=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFSlaSN_ELhZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the model, optimiser and the loss function\n",
        "model=Recognition().to(device)\n",
        "optimiser=optim.Adam(model.parameters(),lr=1e-4)\n",
        "lossfn=nn.CrossEntropyLoss()\n",
        "\n",
        "def calc_loss(pred,labels):\n",
        "    loss=0\n",
        "    for i in range(pred.size(1)):     # seq_length dim\n",
        "        loss+=lossfn(pred[:,i],labels[:,i])\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPlQXAVhd3UB",
        "colab_type": "code",
        "outputId": "75e6c897-f3d9-48ed-c476-59ec472c4aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "# TRAIN THE MODEL\n",
        "epochs=50\n",
        "loss_h=[]\n",
        "\n",
        "for e in range(epoch,epochs):\n",
        "    tot_loss=0\n",
        "    for batch,flabels,rlabels,lengths in train_loader:\n",
        "        batch=batch.to(device)\n",
        "        flabels=flabels.to(device)\n",
        "        rlabels=rlabels.to(device)\n",
        "        lengths=lengths.to(device)\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        fpred,rpred,alphas=model(batch,flabels,rlabels,lengths)   \n",
        "        loss=0.5*(calc_loss(fpred,flabels)+calc_loss(rpred,rlabels))\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "        tot_loss+=loss.item()\n",
        "    \n",
        "    torch.save({\n",
        "            'epoch': e,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimiser_state_dict': optimiser.state_dict(),\n",
        "            'loss_h': loss_h,\n",
        "            }, '/content/drive/My Drive/Scene text recognition/text_recognition_revdecoder_epoch{}.pth'.format(e))\n",
        "    \n",
        "    tot_loss/=len(train_loader)\n",
        "    loss_h.append(tot_loss)\n",
        "    print('Loss in epoch {} = {}'.format(e, tot_loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRaining starTeD\n",
            "loss in epoch 0 = 29.65858753876795\n",
            "\n",
            "loss in epoch 1 = 24.384523937569867\n",
            "\n",
            "loss in epoch 2 = 21.026255617982287\n",
            "\n",
            "loss in epoch 3 = 13.813699123260117\n",
            "\n",
            "loss in epoch 4 = 7.393409310928237\n",
            "\n",
            "loss in epoch 5 = 4.892732756441384\n",
            "\n",
            "loss in epoch 6 = 3.809656375640105\n",
            "\n",
            "loss in epoch 7 = 3.1411799066603767\n",
            "\n",
            "loss in epoch 8 = 2.7166296441809266\n",
            "\n",
            "loss in epoch 9 = 2.383363974399224\n",
            "\n",
            "loss in epoch 10 = 2.1281862535531166\n",
            "\n",
            "loss in epoch 11 = 1.9076894444694197\n",
            "\n",
            "loss in epoch 12 = 1.7328410596628314\n",
            "\n",
            "loss in epoch 13 = 1.5932954653562477\n",
            "\n",
            "loss in epoch 14 = 1.4662350916888431\n",
            "\n",
            "loss in epoch 15 = 1.335463422183373\n",
            "\n",
            "loss in epoch 16 = 1.2656685027645513\n",
            "\n",
            "loss in epoch 17 = 1.1592571245216572\n",
            "\n",
            "loss in epoch 18 = 1.0766785509065524\n",
            "\n",
            "loss in epoch 19 = 0.9982552102587557\n",
            "\n",
            "loss in epoch 20 = 0.9267449543869145\n",
            "\n",
            "loss in epoch 21 = 0.8639588951578078\n",
            "\n",
            "loss in epoch 22 = 0.8272643280058483\n",
            "\n",
            "loss in epoch 23 = 0.74009816303152\n",
            "\n",
            "loss in epoch 24 = 0.7178446544218764\n",
            "\n",
            "loss in epoch 25 = 0.6592548187736977\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8klKKRqFP79-",
        "colab_type": "code",
        "outputId": "bb234507-d1d1-4e77-a247-5da3eed9a36a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "# LOADING THE MODEL\n",
        "PATH='/content/drive/My Drive/Scene text recognition/text_recognition_revdecoder_epoch25.pth'\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimiser.load_state_dict(checkpoint['optimiser_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss_h=checkpoint['loss_h']\n",
        "\n",
        "print(\"Model was saved at epoch:\",epoch)\n",
        "plt.plot(loss_h)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model was saved at epoch: 25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}